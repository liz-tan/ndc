{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDC 3.0 Analysis \n",
    "## Building an NLP pipeline to analyse NDCs and track contributions over time.\n",
    "\n",
    "Countries submit their updated NDCs (NDC 3.0) in 2025. Each round of NDCs should be more ambitious than the last to put us on track to limit warming under 2 degrees. The next [Global Stocktake](https://unfccc.int/documents/631600) will take place in 2028, and is critical for checking progress toward the Paris Agreement. \n",
    "\n",
    "**Given the urgency of climate action required, can we check on the progress towards emissions reductions and resilience in each commitment in real time?**\n",
    "\n",
    "**What can we learn from NDCs to understand how to address financing gaps?**\n",
    "\n",
    "**How do mitigation, adaptation and L&D feature in NDCs?**\n",
    "\n",
    "## Method\n",
    "- Step 1: Collect the NDC Data --> Scrape the NDC Registry for links to NDCs\n",
    "- Step 2: Download the NDC texts --> Download from the scraped links\n",
    "- Step 3: Preprocess the Text Data --> Convert unstructured NDC text into clean, structured format for analysis.\n",
    "- Step 4: Compare Ambition Over Time --> Track changes in emissions reduction targets over time.\n",
    "- Step 5: Identify Key Sectors & Financial Instruments --> Extract mentions of economic sectors (e.g., energy, transport) and financial instruments (e.g., carbon pricing, green bonds).\n",
    "- Step 6: Visualise the Findings on a Global Map --> Show how each country's ambition score has changed across NDC updates.\n",
    "- Step 7: Automate & Scale the Pipeline --> Make this process reproducible for future NDC updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect the NDC Data\n",
    "# Check for bulk download of NDCs\n",
    "!pip install requests beautifulsoup4 pandas tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for the NDC Registry\n",
    "ndc_url = \"https://unfccc.int/NDCREG\"\n",
    "\n",
    "# Send a request to fetch the webpage\n",
    "response = requests.get(ndc_url)\n",
    "\n",
    "# Check if the request was successful (Status Code 200 = OK)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully accessed the NDC Registry page!\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # Parse the HTML\n",
    "else:\n",
    "    print(f\"Failed to access the page. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape the NDC Registry page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no browser window)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Start a new browser session\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open the UNFCCC NDC Registry page\n",
    "driver.get(\"https://unfccc.int/NDCREG\")\n",
    "time.sleep(5)  # Wait for JavaScript to load the page\n",
    "\n",
    "# Scroll down to load more content (if necessary)\n",
    "scroll_pause_time = 2  # Pause time to allow the page to load content\n",
    "scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    new_scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_scroll_height == scroll_height:\n",
    "        break\n",
    "    scroll_height = new_scroll_height\n",
    "\n",
    "# Find all <a> tags on the page\n",
    "all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "# Filter out the links that lead to PDF documents (based on the .pdf file extension)\n",
    "ndc_urls = [link.get_attribute(\"href\") for link in all_links if link.get_attribute(\"href\") and link.get_attribute(\"href\").endswith(\".pdf\")]\n",
    "\n",
    "# Close the browser session\n",
    "driver.quit()\n",
    "\n",
    "# Show the first 10 NDC document links\n",
    "print(f\"Found {len(ndc_urls)} NDC document links.\")\n",
    "print(ndc_urls[:10])  # Display first 10 links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique PDF links\n",
    "ndc_urls = list(set(ndc_urls))  # Remove duplicates by converting to a set\n",
    "\n",
    "# Show the cleaned-up number of links\n",
    "print(f\"Found {len(ndc_urls)} unique NDC document links.\")\n",
    "print(ndc_urls[:10])  # Display first 10 links to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download the NDCs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set the download directory (modify this path to your actual folder)\n",
    "download_directory = os.path.expanduser(\"~/Desktop/Projects/NDC/ndc_downloads\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "# Configure Chrome options for automatic PDF download\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_directory,  # Set default download folder\n",
    "    \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,  # Bypass Chrome's PDF viewer\n",
    "})\n",
    "\n",
    "# Start a new browser session\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Visit each NDC link and download the PDF\n",
    "for pdf_url in ndc_urls:\n",
    "    print(f\"Downloading: {pdf_url}\")\n",
    "    driver.get(pdf_url)\n",
    "    time.sleep(5)  # Wait for the file to download\n",
    "\n",
    "# Close the browser session\n",
    "driver.quit()\n",
    "\n",
    "print(\"All downloads completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the Text Data\n",
    "!pip install pdfplumber\n",
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A: Preprocess the Text Data --> store in list version\n",
    "# Path where your NDC PDFs are saved\n",
    "pdf_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_downloads\"\n",
    "\n",
    "# Create a list to hold the extracted data\n",
    "ndc_data = []\n",
    "\n",
    "# Iterate through all PDFs in the folder\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_folder, file_name)\n",
    "        \n",
    "        # Open the PDF and extract text\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        \n",
    "        # Clean up and store the text\n",
    "        ndc_data.append({\"country\": file_name, \"text\": text})\n",
    "\n",
    "# Preview the extracted data\n",
    "print(ndc_data[:2])  # Show the first 2 entries for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B: Preprocess the Text Data --> store as txt version for later use\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "# Folder where PDFs are stored\n",
    "pdf_folder = \"Desktop/Projects/NDC/ndc_downloads\"\n",
    "text_folder = \"Desktop/Projects/NDC/ndc_texts\"  \n",
    "\n",
    "# Ensure the text output folder exists\n",
    "os.makedirs(text_folder, exist_ok=True)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Save cleaned text to a file\n",
    "        text_file_path = os.path.join(text_folder, pdf_file.replace(\".pdf\", \".txt\"))\n",
    "        with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(text)\n",
    "        \n",
    "        print(f\"Extracted text from {pdf_file} and saved to {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ndc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Process each NDC document\u001b[39;00m\n\u001b[32m     16\u001b[39m structured_data = []\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mndc_data\u001b[49m:\n\u001b[32m     19\u001b[39m     country = entry[\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m     text = entry[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'ndc_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Compare Ambition Over Time\n",
    "\n",
    "import re\n",
    "\n",
    "# Define keywords related to our three topics\n",
    "emissions_keywords = [\"emission\", \"carbon\", \"CO2\", \"GHG\", \"reduction\", \"net-zero\", \"mitigation\"]\n",
    "sector_keywords = [\"energy\", \"transport\", \"agriculture\", \"industry\", \"waste\", \"forestry\", \"buildings\"]\n",
    "finance_keywords = [\"investment\", \"funding\", \"finance\", \"carbon market\", \"green bond\", \"climate finance\"]\n",
    "\n",
    "# Function to extract relevant info\n",
    "def extract_info(text, keywords):\n",
    "    matches = [word for word in keywords if re.search(rf\"\\b{word}\\b\", text, re.IGNORECASE)]\n",
    "    return list(set(matches))  # Remove duplicates\n",
    "\n",
    "# Process each NDC document\n",
    "structured_data = []\n",
    "\n",
    "for entry in ndc_data:\n",
    "    country = entry[\"country\"]\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    # Extract relevant terms\n",
    "    emissions_terms = extract_info(text, emissions_keywords)\n",
    "    sector_terms = extract_info(text, sector_keywords)\n",
    "    finance_terms = extract_info(text, finance_keywords)\n",
    "\n",
    "    structured_data.append({\n",
    "        \"country\": country,\n",
    "        \"emissions_focus\": emissions_terms,\n",
    "        \"sectors_mentioned\": sector_terms,\n",
    "        \"finance_mentions\": finance_terms\n",
    "    })\n",
    "\n",
    "# Show a preview of the results\n",
    "print(structured_data[:3])  # First 3 results for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
