{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDC 3.0 Analysis \n",
    "## Building an NLP pipeline to analyse NDCs and track contributions over time.\n",
    "\n",
    "Countries submit their updated NDCs (NDC 3.0) in 2025. Each round of NDCs should be more ambitious than the last to put us on track to limit warming under 2 degrees. The next [Global Stocktake](https://unfccc.int/documents/631600) will take place in 2028, and is critical for checking progress toward the Paris Agreement. \n",
    "\n",
    "**Given the urgency of climate action required, can we check on the progress towards emissions reductions and resilience in each commitment in real time?**\n",
    "\n",
    "**What can we learn from NDCs to understand how to address financing gaps?**\n",
    "\n",
    "**How do mitigation, adaptation and L&D feature in NDCs, particularly, what are the needs and priorities of developing countries?**\n",
    "\n",
    "## Method\n",
    "- Step 1: Collect the NDC Data --> Scrape the NDC Registry for links to NDCs\n",
    "- Step 2: Download the NDC texts --> Download from the scraped links\n",
    "- Step 3: Preprocess the Text Data --> Convert unstructured NDC text into clean, structured format for analysis.\n",
    "- Step 4: Compare Ambition Over Time --> Track changes in emissions reduction targets over time.\n",
    "- Step 5: Identify Key Sectors & Financial Instruments --> Extract mentions of economic sectors (e.g., energy, transport) and financial instruments (e.g., carbon pricing, green bonds).\n",
    "- Step 6: Visualise the Findings on a Global Map --> Show how each country's ambition score has changed across NDC updates.\n",
    "- Step 7: Automate & Scale the Pipeline --> Make this process reproducible for future NDC updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'venv (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!which python\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install numpy --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect the NDC Data\n",
    "# Check for bulk download of NDCs\n",
    "!pip install requests beautifulsoup4 pandas tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for the NDC Registry\n",
    "ndc_url = \"https://unfccc.int/NDCREG\"\n",
    "\n",
    "# Send a request to fetch the webpage\n",
    "response = requests.get(ndc_url)\n",
    "\n",
    "# Check if the request was successful (Status Code 200 = OK)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully accessed the NDC Registry page!\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # Parse the HTML\n",
    "else:\n",
    "    print(f\"Failed to access the page. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape the NDC Registry page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no browser window)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Start a new browser session\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open the UNFCCC NDC Registry page\n",
    "driver.get(\"https://unfccc.int/NDCREG\")\n",
    "time.sleep(5)  # Wait for JavaScript to load the page\n",
    "\n",
    "# Scroll down to load more content (if necessary)\n",
    "scroll_pause_time = 2  # Pause time to allow the page to load content\n",
    "scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    new_scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_scroll_height == scroll_height:\n",
    "        break\n",
    "    scroll_height = new_scroll_height\n",
    "\n",
    "# Find all <a> tags on the page\n",
    "all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "# Filter out the links that lead to PDF documents (based on the .pdf file extension)\n",
    "ndc_urls = [link.get_attribute(\"href\") for link in all_links if link.get_attribute(\"href\") and link.get_attribute(\"href\").endswith(\".pdf\")]\n",
    "\n",
    "# Close the browser session\n",
    "driver.quit()\n",
    "\n",
    "# Show the first 10 NDC document links\n",
    "print(f\"Found {len(ndc_urls)} NDC document links.\")\n",
    "print(ndc_urls[:10])  # Display first 10 links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique PDF links\n",
    "ndc_urls = list(set(ndc_urls))  # Remove duplicates by converting to a set\n",
    "\n",
    "# Show the cleaned-up number of links\n",
    "print(f\"Found {len(ndc_urls)} unique NDC document links.\")\n",
    "print(ndc_urls[:10])  # Display first 10 links to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download the NDCs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set the download directory (modify this path to your actual folder)\n",
    "download_directory = os.path.expanduser(\"~/Desktop/Projects/NDC/ndc_downloads\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "# Configure Chrome options for automatic PDF download\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"prefs\", {\n",
    "    \"download.default_directory\": download_directory,  # Set default download folder\n",
    "    \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True,  # Bypass Chrome's PDF viewer\n",
    "})\n",
    "\n",
    "# Start a new browser session\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Visit each NDC link and download the PDF\n",
    "for pdf_url in ndc_urls:\n",
    "    print(f\"Downloading: {pdf_url}\")\n",
    "    driver.get(pdf_url)\n",
    "    time.sleep(5)  # Wait for the file to download\n",
    "\n",
    "# Close the browser session\n",
    "driver.quit()\n",
    "\n",
    "print(\"All downloads completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall cryptography -y\n",
    "!pip install cryptography --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the Text Data\n",
    "!pip install --upgrade pip\n",
    "!pip install pdfplumber\n",
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3A: Preprocess the Text Data --> store in list version\n",
    "# Path where your NDC PDFs are saved\n",
    "pdf_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_downloads\"\n",
    "\n",
    "# Create a list to hold the extracted data\n",
    "ndc_data = []\n",
    "\n",
    "# Iterate through all PDFs in the folder\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_folder, file_name)\n",
    "        \n",
    "        # Open the PDF and extract text\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        \n",
    "        # Clean up and store the text\n",
    "        ndc_data.append({\"country\": file_name, \"text\": text})\n",
    "\n",
    "# Preview the extracted data\n",
    "print(ndc_data[:2])  # Show the first 2 entries for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B: Preprocess the Text Data --> store as list\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# Path to where PDFs are saved\n",
    "pdf_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_downloads\"\n",
    "\n",
    "# List to store extracted text\n",
    "ndc_data = []\n",
    "\n",
    "# Iterate through all PDFs in the folder\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_folder, file_name)\n",
    "        \n",
    "        # Open the PDF and extract text\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "        \n",
    "        # Store extracted data\n",
    "        ndc_data.append({\"country\": file_name.replace(\".pdf\", \"\"), \"text\": text})\n",
    "\n",
    "# Confirm extraction worked\n",
    "print(f\"Extracted {len(ndc_data)} documents\")\n",
    "print(ndc_data[:2])  # Show first two entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3B: option Preprocess the Text Data --> store as txt version for later use\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "# Folder where PDFs are stored\n",
    "pdf_folder = \"Desktop/Projects/NDC/ndc_downloads\"\n",
    "text_folder = \"Desktop/Projects/NDC/ndc_texts\"  \n",
    "\n",
    "# Ensure the text output folder exists\n",
    "os.makedirs(text_folder, exist_ok=True)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Save cleaned text to a file\n",
    "        text_file_path = os.path.join(text_folder, pdf_file.replace(\".pdf\", \".txt\"))\n",
    "        with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(text)\n",
    "        \n",
    "        print(f\"Extracted text from {pdf_file} and saved to {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract relevant information from NDCs\n",
    "\n",
    "import re\n",
    "\n",
    "# Define keywords related to our three topics\n",
    "emissions_keywords = [\"emission\", \"carbon\", \"CO2\", \"GHG\", \"reduction\", \"net-zero\", \"mitigation\"]\n",
    "sector_keywords = [\"energy\", \"transport\", \"agriculture\", \"industry\", \"waste\", \"forestry\", \"buildings\"]\n",
    "finance_keywords = [\"investment\", \"funding\", \"finance\", \"carbon market\", \"green bond\", \"climate finance\"]\n",
    "\n",
    "# Function to extract relevant info\n",
    "def extract_info(text, keywords):\n",
    "    matches = [word for word in keywords if re.search(rf\"\\b{word}\\b\", text, re.IGNORECASE)]\n",
    "    return list(set(matches))  # Remove duplicates\n",
    "\n",
    "# Process each NDC document\n",
    "structured_data = []\n",
    "\n",
    "for entry in ndc_data:\n",
    "    country = entry[\"country\"]\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    # Extract relevant terms\n",
    "    emissions_terms = extract_info(text, emissions_keywords)\n",
    "    sector_terms = extract_info(text, sector_keywords)\n",
    "    finance_terms = extract_info(text, finance_keywords)\n",
    "\n",
    "    structured_data.append({\n",
    "        \"country\": country,\n",
    "        \"emissions_focus\": emissions_terms,\n",
    "        \"sectors_mentioned\": sector_terms,\n",
    "        \"finance_mentions\": finance_terms\n",
    "    })\n",
    "\n",
    "# Show a preview of the results\n",
    "print(structured_data[:3])  # First 3 results for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Identify Key Sectors & Financial Instruments by country\n",
    "from collections import defaultdict\n",
    "\n",
    "# Organize data by country\n",
    "country_versions = defaultdict(list)\n",
    "\n",
    "for entry in structured_data:\n",
    "    country_name = re.sub(r'[^a-zA-Z]', '', entry[\"country\"].split(\"_\")[0])  # Extract country name\n",
    "    country_versions[country_name].append(entry)\n",
    "\n",
    "# Show sample country entries\n",
    "for country, versions in list(country_versions.items())[:5]:  # Show only 5 examples\n",
    "    print(f\"{country}: {len(versions)} versions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by UNFCCC Party Groupings\n",
    "\n",
    "import pandas as pd\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Define UNFCCC Party Groupings\n",
    "party_groupings = {\n",
    "    \"African Group\": [\"Algeria\", \"Angola\", \"Benin\", \"Botswana\", \"Burkina Faso\", \"Burundi\", \"Cabo Verde\", \"Cameroon\", \"Central African Republic\", \"Chad\", \"Comoros\", \"Congo\", \"Democratic Republic of the Congo\", \"Djibouti\", \"Egypt\", \"Equatorial Guinea\", \"Eritrea\", \"Eswatini\", \"Ethiopia\", \"Gabon\", \"Gambia\", \"Ghana\", \"Guinea\", \"Guinea-Bissau\", \"Ivory Coast\", \"Kenya\", \"Lesotho\", \"Liberia\", \"Libya\", \"Madagascar\", \"Malawi\", \"Mali\", \"Mauritania\", \"Mauritius\", \"Morocco\", \"Mozambique\", \"Namibia\", \"Niger\", \"Nigeria\", \"Rwanda\", \"Sao Tome and Principe\", \"Senegal\", \"Seychelles\", \"Sierra Leone\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\", \"Tanzania\", \"Togo\", \"Tunisia\", \"Uganda\", \"Zambia\", \"Zimbabwe\"],\n",
    "    \"AOSIS\": [\"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Comoros\", \"Cook Islands\", \"Cuba\", \"Dominica\", \"Dominican Republic\", \"Fiji\", \"Grenada\", \"Guinea-Bissau\", \"Guyana\", \"Haiti\", \"Jamaica\", \"Kiribati\", \"Maldives\", \"Marshall Islands\", \"Mauritius\", \"Micronesia\", \"Nauru\", \"Niue\", \"Palau\", \"Papua New Guinea\", \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\", \"Samoa\", \"Sao Tome and Principe\", \"Seychelles\", \"Singapore\", \"Solomon Islands\", \"Suriname\", \"Timor-Leste\", \"Tonga\", \"Trinidad and Tobago\", \"Tuvalu\", \"Vanuatu\"],\n",
    "    \"LDCs\": [\"Afghanistan\", \"Angola\", \"Bangladesh\", \"Benin\", \"Bhutan\", \"Burkina Faso\", \"Burundi\", \"Cambodia\", \"Central African Republic\", \"Chad\", \"Comoros\", \"Democratic Republic of the Congo\", \"Djibouti\", \"Eritrea\", \"Ethiopia\", \"Gambia\", \"Guinea\", \"Guinea-Bissau\", \"Haiti\", \"Kiribati\", \"Laos\", \"Lesotho\", \"Liberia\", \"Madagascar\", \"Malawi\", \"Mali\", \"Mauritania\", \"Mozambique\", \"Myanmar\", \"Nepal\", \"Niger\", \"Rwanda\", \"Sao Tome and Principe\", \"Senegal\", \"Sierra Leone\", \"Solomon Islands\", \"Somalia\", \"South Sudan\", \"Sudan\", \"Tanzania\", \"Timor-Leste\", \"Togo\", \"Tuvalu\", \"Uganda\", \"Yemen\", \"Zambia\"],\n",
    "    \"Umbrella Group\": [\"Australia\", \"Canada\", \"Iceland\", \"Israel\", \"Japan\", \"New Zealand\", \"Norway\", \"Ukraine\", \"United States\"]\n",
    "}\n",
    "\n",
    "# Step 2: Assign Each Country to a Group\n",
    "def assign_group(country_name):\n",
    "    for group, countries in party_groupings.items():\n",
    "        if any(country_name.startswith(c) for c in countries):\n",
    "            return group\n",
    "    return \"Other\"\n",
    "\n",
    "# Convert extracted data into a DataFrame\n",
    "df = pd.DataFrame(structured_data)\n",
    "df[\"grouping\"] = df[\"country\"].apply(assign_group)\n",
    "\n",
    "# Step 3: Aggregate Mentions by Group\n",
    "def count_mentions(column):\n",
    "    mention_counts = defaultdict(int)\n",
    "    for _, row in df.iterrows():\n",
    "        for term in row[column]:\n",
    "            mention_counts[row[\"grouping\"]] += 1\n",
    "    return mention_counts\n",
    "\n",
    "emissions_count = count_mentions(\"emissions_focus\")\n",
    "sectors_count = count_mentions(\"sectors_mentioned\")\n",
    "finance_count = count_mentions(\"finance_mentions\")\n",
    "\n",
    "# Step 4: Visualizing the Results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Emissions Mentions\n",
    "axes[0].bar(emissions_count.keys(), emissions_count.values(), color='b')\n",
    "axes[0].set_title(\"Emissions-related Mentions by Group\")\n",
    "axes[0].set_xticklabels(emissions_count.keys(), rotation=45, ha=\"right\")\n",
    "\n",
    "# Sector Mentions\n",
    "axes[1].bar(sectors_count.keys(), sectors_count.values(), color='g')\n",
    "axes[1].set_title(\"Sector-related Mentions by Group\")\n",
    "axes[1].set_xticklabels(sectors_count.keys(), rotation=45, ha=\"right\")\n",
    "\n",
    "# Finance Mentions\n",
    "axes[2].bar(finance_count.keys(), finance_count.values(), color='r')\n",
    "axes[2].set_title(\"Finance-related Mentions by Group\")\n",
    "axes[2].set_xticklabels(finance_count.keys(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: run through LLM to create structured summaries by negotiating blocs \n",
    "\n",
    "# # how many NDCs include plans for adaptation and L&D?\n",
    "# Upload in batches to NotebookLM to read the NDCs and create structured summaries by negotiating blocs \n",
    "# Take summaries \n",
    "\n",
    "!pip install openai\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key-here\"  # You can store it in an environment variable for security\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: run through local LLM\n",
    "# download 0llama: https://ollama.com/download\n",
    "# Do in terminal:\n",
    "ollama pull mistral\n",
    "ollama run mistral\n",
    "\n",
    "# get api key from ollama\n",
    "ollama serve\n",
    "\n",
    "# check that it's running\n",
    "curl http://localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text discusses climate change as a global issue or challenge. However, it does not provide specific details about the context, causes, or solutions related to climate change in this summary.\n"
     ]
    }
   ],
   "source": [
    "# test back here\n",
    "# import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"mistral\", \n",
    "        \"prompt\": \"Summarize this text: Climate change is a global challenge...\",\n",
    "        \"stream\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_downloads\"\n",
    "txt_output_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_txts\"\n",
    "\n",
    "os.makedirs(txt_output_folder, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        txt_path = os.path.join(txt_output_folder, file_name.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "            \n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {file_name}: {e}\")\n",
    "\n",
    "# run the model with a specific prompt\n",
    "ollama run mistral --prompt \"You are an international climate policy expert conducting an analysis of the content in these NDCs by their context, goals, implementation attributes and actions. I will define these categories for you. Context: key needs, key vulnerabilities, policy considerations (adaptation-specific or broader climate and development policies), stakeholder engagement. Goals: stated climate goals, broad targets, specific targets, sector identification. Implementation attributes: transparency, means of implementation (per the UNFCCC definition), implementation process, integration. Actions (aligned with the IPCC 7th assessment report): structural and physical, social, institutional and sectoral.\"\n",
    "pdf_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_downloads\"\n",
    "txt_output_folder = \"/Users/liztan/Desktop/Projects/NDC/ndc_txts\"\n",
    "\n",
    "os.makedirs(txt_output_folder, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(pdf_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, file_name)\n",
    "        txt_path = os.path.join(txt_output_folder, file_name.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "            \n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ndc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Your extracted NDC data (from pdfplumber)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example: ndc_data = [{\"country\": \"Kenya.pdf\", \"text\": \"Full NDC text...\"}, ...]\u001b[39;00m\n\u001b[32m      6\u001b[39m structured_ndc_summaries = []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mndc_data\u001b[49m:\n\u001b[32m      9\u001b[39m     country = entry[\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m     text = entry[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m8000\u001b[39m]  \u001b[38;5;66;03m# Truncate to keep within token limits\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ndc_data' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Your extracted NDC data (from pdfplumber)\n",
    "# Example: ndc_data = [{\"country\": \"Kenya.pdf\", \"text\": \"Full NDC text...\"}, ...]\n",
    "\n",
    "structured_ndc_summaries = []\n",
    "\n",
    "for entry in ndc_data:\n",
    "    country = entry[\"country\"]\n",
    "    text = entry[\"text\"][:8000]  # Truncate to keep within token limits\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following NDC document for {country}.\n",
    "    Focus on:\n",
    "    1. Emissions targets\n",
    "    2. Sectoral focus (e.g., energy, agriculture, waste, etc.)\n",
    "    3. Climate finance, investments, or market mechanisms\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"mistral\",  # or \"llama3\" or other pulled model\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = response.json()[\"response\"]\n",
    "    structured_ndc_summaries.append({\n",
    "        \"country\": country,\n",
    "        \"summary\": result\n",
    "    })\n",
    "\n",
    "# Example: view one result\n",
    "print(structured_ndc_summaries[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
